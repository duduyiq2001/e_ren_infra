# Karpenter Configuration for E-Ren EKS Cluster
#
# Apply after cluster is created:
#   aws eks update-kubeconfig --region us-east-1 --name e-ren-cluster
#   kubectl apply -f karpenter-config.yaml
#
# Architecture:
# - 1 fixed on-demand node (always running, hosts Rails app + Karpenter)
# - Karpenter provisions 0-2 spot nodes for burst traffic
#
# This creates:
# - EC2NodeClass (infrastructure config for spot nodes)
# - Spot NodePool (autoscales 0-2 nodes based on load)

---
apiVersion: karpenter.k8s.aws/v1
kind: EC2NodeClass
metadata:
  name: default
spec:
  # AMI family (Karpenter auto-selects latest)
  amiFamily: AL2

  # IAM role for nodes (created by Karpenter module)
  # NOTE: Replace ${CLUSTER_NAME} with actual cluster name
  role: e-ren-cluster

  # Subnet selector (uses private subnets tagged for Karpenter)
  subnetSelectorTerms:
    - tags:
        karpenter.sh/discovery: e-ren-cluster

  # Security group selector
  securityGroupSelectorTerms:
    - tags:
        karpenter.sh/discovery: e-ren-cluster

  # User data for node initialization
  userData: |
    #!/bin/bash
    /etc/eks/bootstrap.sh e-ren-cluster

  # Block device mappings (root volume)
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 20Gi
        volumeType: gp3
        encrypted: true
        deleteOnTermination: true

  # Metadata options (IMDSv2 required for security)
  metadataOptions:
    httpEndpoint: enabled
    httpProtocolIPv6: disabled
    httpPutResponseHopLimit: 2
    httpTokens: required

---
# No on-demand NodePool needed - we have a fixed on-demand node group
# Karpenter only manages spot instances for burst capacity


apiVersion: karpenter.sh/v1
kind: NodePool
metadata:
  name: spot
spec:
  # Template for nodes
  template:
    metadata:
      labels:
        workload-type: batch
        capacity-type: spot

    spec:
      # Use default EC2NodeClass
      nodeClassRef:
        group: karpenter.k8s.aws
        kind: EC2NodeClass
        name: default

      # Requirements for node selection
      requirements:
        # Use spot instances only
        - key: karpenter.sh/capacity-type
          operator: In
          values: ["spot"]

        # ARM instances (cheaper + better spot availability)
        - key: kubernetes.io/arch
          operator: In
          values: ["arm64"]

        # Diverse instance types for better spot availability
        - key: node.kubernetes.io/instance-type
          operator: In
          values:
            - t4g.small
            - t4g.medium
            - t4g.large
            - c6g.medium
            - c6g.large
            - m6g.medium
            - m6g.large

        # Availability zones (update to match your AWS region)
        - key: topology.kubernetes.io/zone
          operator: In
          values:
            - us-east-1a
            - us-east-1b

      # Taint spot nodes so only tolerant pods schedule here
      taints:
        - key: spot
          value: "true"
          effect: NO_SCHEDULE

  # Limits: Cap at ~2 t4g.medium instances (2 vCPU, 4Gi RAM each)
  # Karpenter will provision nodes as needed up to this limit
  limits:
    cpu: "4"
    memory: 8Gi

  # Disruption budget (aggressive consolidation for cost savings)
  disruption:
    consolidationPolicy: WhenEmpty
    consolidateAfter: 30s
    # Allow spot interruptions
    budgets:
      - nodes: "100%"
